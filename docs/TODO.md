# TODO
* first_network_ver5 -> note about derivative -> look into it
* derivative of log loss function
* How are those dividing lines in graph connected to weights and biases
* error derivative for log_loss
* hidden layer error derivative -> why transpose matrix and element wise derivative of activation function (why not division)
* nonlinear example => try with leaky ReLu
* speeding up things using numPy, then using TensorFlow or PyTorch